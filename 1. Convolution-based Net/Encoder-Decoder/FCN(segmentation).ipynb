{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully Convolutional Networks for Semantic Segmentation**   \n",
    "*Jonathan Long, Evan Shelhamer, and Trevor Darrell*   \n",
    "[[arXiv]] [arXiv]: https://arxiv.org/abs/1411.4038   \n",
    "CoRR 2014 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN32(VGG):\n",
    "    def __init__(self):\n",
    "        super(FCN32, self).__init__(make_layers(cfg['vgg16']))\n",
    "\n",
    "        self.numclass = 21\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(512, 4096, kernel_size=7)\n",
    "        self.conv2 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
    "\n",
    "        self.classifier = nn.Conv2d(4096, self.numclass, kernel_size=1, stride=1, padding=0)\n",
    "        self.upsampler = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=64, stride=32, bias=False)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def load_pretrained(self, pretrained_model):\n",
    "        \n",
    "        self.features = pretrained_model.features\n",
    "        fc6 = pretrained_model.classifier[0]\n",
    "        fc7 = pretrained_model.classifier[3]\n",
    "\n",
    "        conv1W = nn.parameter.Parameter(fc6.weight.view(4096,512,7,7))\n",
    "        conv1B =  nn.parameter.Parameter(fc6.bias) # bias(C_{out_dim})\n",
    "        conv2W = nn.parameter.Parameter(fc7.weight.view(4096,4096,1,1))\n",
    "        conv2B =  nn.parameter.Parameter(fc7.bias)\n",
    "\n",
    "        # for the pre-trained weights of VGG16\n",
    "        with torch.no_grad():\n",
    "          self.conv1.weight = conv1W\n",
    "          self.conv1.bias   = conv1B\n",
    "          self.conv2.weight = conv2W\n",
    "          self.conv2.bias   = conv2B\n",
    "\n",
    "    def vgg_layer_forward(self, x, indices):\n",
    "        output = x\n",
    "        start_idx, end_idx = indices\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            output = self.features[idx](output)\n",
    "        return output\n",
    "\n",
    "    def vgg_forward(self, x):\n",
    "        out = {}\n",
    "        layer_indices = [0, 5, 10, 17, 24, 31]\n",
    "        for layer_num in range(len(layer_indices)-1):\n",
    "            x = self.vgg_layer_forward(x, layer_indices[layer_num:layer_num+2])\n",
    "            out[f'pool{layer_num+1}'] = x\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Padding for aligning to the input size\n",
    "        padded_x = F.pad(x, [100, 100, 100, 100], \"constant\", 0)\n",
    "        vgg_features = self.vgg_forward(padded_x)\n",
    "        vgg_pool5 = vgg_features['pool5'].detach()\n",
    "        vgg_pool4 = vgg_features['pool4'].detach()\n",
    "        vgg_pool3 = vgg_features['pool3'].detach()\n",
    "\n",
    "        h = self.conv1(vgg_pool5)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        classified = self.classifier(h)\n",
    "        upsampled = self.upsampler(classified)\n",
    "        out = transforms.functional.crop(upsampled, top=9, left=9, height=x.shape[-2], width=x.shape[-1])\n",
    "\n",
    "        return out\n",
    "\n",
    "    # initialize transdeconv layer with bilinear upsampling.\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN8(FCN32):\n",
    "    def __init__(self):\n",
    "        super(FCN8, self).__init__()\n",
    "\n",
    "        self.numclass = 21\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "\n",
    "        # inheried\n",
    "        # self.conv1 = nn.Conv2d(512, 4096, kernel_size=7)\n",
    "        # self.conv2 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
    "\n",
    "        self.classifier1 = nn.Conv2d(4096, self.numclass, kernel_size=1)\n",
    "        self.classifier2 = nn.Conv2d(512, self.numclass, kernel_size=1)\n",
    "        self.classifier3 = nn.Conv2d(256, self.numclass, kernel_size=1)\n",
    "\n",
    "        # Learnable upsampling layers in FCN model.\n",
    "        self.trans1 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=4, stride=2, bias=False)\n",
    "        self.trans2 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=4, stride=2, bias=False)\n",
    "        self.trans3 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "        # initialize deconv layer with bilinear upsampling.\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Padding for aligning to the input size\n",
    "        padded_x = F.pad(x, [100, 100, 100, 100], \"constant\", 0)\n",
    "        vgg_features = self.vgg_forward(padded_x)\n",
    "        vgg_pool5 = vgg_features['pool5'].detach()\n",
    "        vgg_pool4 = vgg_features['pool4'].detach()\n",
    "        vgg_pool3 = vgg_features['pool3'].detach()\n",
    "\n",
    "        h = self.conv1(vgg_pool5)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        h2 = 0.01 * vgg_pool4\n",
    "        h3 = 0.0001 * vgg_pool3 \n",
    "\n",
    "        h1 = self.classifier1(h)\n",
    "        h2 = self.classifier2(h2)\n",
    "        h3 = self.classifier3(h3)\n",
    "\n",
    "        h1 = self.trans1(h1)\n",
    "        h2 = transforms.functional.crop(h2, top=5, left=5, height=h1.shape[-2], width=h1.shape[-1])\n",
    "        h1 = h1 + h2 \n",
    "\n",
    "        h1 = self.trans2(h1)\n",
    "        h3 = transforms.functional.crop(h3, top=9, left=9, height=h1.shape[-2], width=h1.shape[-1])\n",
    "        h1 = h1 + h3 \n",
    "\n",
    "        h1 = self.trans3(h1)\n",
    "        out = transforms.functional.crop(h1, top=31, left=31, height=x.shape[-2], width=x.shape[-1])       \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the pre-trained model\n",
    "# Load pretrained weights.\n",
    "pretrained_path = Path(root) / 'pretrained_vgg.pt'\n",
    "pretrained_VGG = vgg16().to(device)\n",
    "pretrained_VGG.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
    "\n",
    "# Select model here.\n",
    "model = FCN8().to(device)\n",
    "# model = FCN32().to(device)\n",
    "\n",
    "# Load pretrained weights here.\n",
    "model.load_pretrained(pretrained_VGG)\n",
    "\n",
    "# Define optimizer.\n",
    "# According to FCN paper, we doubled the learning rate of bias compared to that of weight.\n",
    "optimizer = SGD([{'params': get_parameters(model, True), 'lr': args.lr * 2, 'weight_decay': 0},\n",
    "                 {'params': get_parameters(model, False)}],\n",
    "                lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28d92f9485c9746d004c1b13759b6237953046e8a4bb9fdcba8f1d233ab2caab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
