{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAMs) Learning Deep Features for Discriminative Localization**   \n",
    "*Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba*   \n",
    "[[paper](https://arxiv.org/abs/1512.04150)]    \n",
    "CVPR 2016  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.datasets import CIFAR100, CIFAR10, imagenet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for VGGnet, we removed the layers after conv5-3 (i.e., pool5 to prb) resulting in a mapping resolution of 14 x 14\n",
    "class CAMs_VGG16(nn.Module):\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super(CAMs_VGG16, self).__init__()\n",
    "\n",
    "        # to remove the last pooling layer\n",
    "        # alternative --> nn.Sequential(*list(model.features.children())[:-1])\n",
    "        self.backbone = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*[self.backbone.features[i] for i in range(len(self.backbone.features)-1)])\n",
    "\n",
    "        # we added a convolutional layer of size 3x3, stride 1, pad 1 with 1024 units, followed by a GAP layer and a softmax layer.\n",
    "        self.conv    = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear  = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # CNN encoder\n",
    "        h = self.backbone(x)\n",
    "        f = self.conv(h) # (B, 1024, w, h)\n",
    "        f = F.relu(f)\n",
    "        p = self.avgpool(f).view(x.shape[0], -1)\n",
    "        out = self.linear(p)\n",
    "\n",
    "        return out, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device\n",
    "\n",
    "# Define train/test data loaders  \n",
    "# Use data augmentation in training set to mitigate overfitting. \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),                                \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([       \n",
    "    transforms.Resize(224),                \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "train_dataset = CIFAR10('dataset/cifar10', download=True, train=True, transform=train_transform)\n",
    "test_dataset = CIFAR10('dataset/cifar10', download=True, train=False, transform=test_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, optimizer, criterion):\n",
    "    global_step = 0\n",
    "    best_accuracy = 0\n",
    "\n",
    "    epochs = 20\n",
    "\n",
    "    net.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Here starts the train loop.\n",
    "        for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            #  Send `x` and `y` to either cpu or gpu using `device` variable. \n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            logit, _ = net(x)\n",
    "\n",
    "            accuracy = (logit.argmax(1) == y).float().mean()\n",
    "            loss = criterion(logit, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.\n",
    "            test_accuracy = 0.\n",
    "            test_num_data = 0.\n",
    "            for batch_idx, (x, y) in tqdm(enumerate(test_dataloader)):\n",
    "                x = x.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "\n",
    "                logit, _ = net(x)\n",
    "\n",
    "                loss = criterion(logit, y)\n",
    "\n",
    "                accuracy = (logit.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "                test_loss += loss.item()*x.shape[0]\n",
    "                test_accuracy += accuracy.item()*x.shape[0]\n",
    "                test_num_data += x.shape[0]\n",
    "\n",
    "            test_loss /= test_num_data\n",
    "            test_accuracy /= test_num_data\n",
    "\n",
    "            print(f'Test result of epoch {epoch}/{epochs} || loss : {test_loss:.3f} acc : {test_accuracy:.3f} ')\n",
    "\n",
    "        # scheduler.step()\n",
    "    return best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAMs_VGG16(num_classes=10)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_accuracy = train_net(model, optimizer,criterion)\n",
    "\n",
    "torch.save(model.state_dict(), 'E:/git/model-implementation/Convolution/model/CAMs_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('E:/git/model-implementation/Convolution/model/CAMs_weights.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_label = test_dataset[0]\n",
    "pred, feature = model(test_img.to(device).view(1, test_img.shape[0], test_img.shape[1], test_img.shape[2]))\n",
    "weights = model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_CAM(feature_conv, weight, class_idx):\n",
    "    size_upsample = (256, 256)\n",
    "    b, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        beforeDot =  feature_conv.reshape((nc, h*w))\n",
    "        cam = np.matmul(weight[idx], beforeDot)\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: 3, pred label: 4\n",
      "img size: torch.Size([3, 224, 224])\n",
      "feature map size: torch.Size([1024, 14, 14]), weights shape: torch.Size([10, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(f'class label: {test_label}, pred label: {pred.argmax(1).item()}')\n",
    "print(f'img size: {test_img.shape}')\n",
    "print(f'feature map size: {feature[0].shape}, weights shape: {weights.shape}')\n",
    "\n",
    "CAMs = return_CAM(feature.to('cpu').detach().numpy(), weights.to('cpu').detach().numpy(), [test_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "test_img = (inv_normalize(test_img).numpy() * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output CAM.jpg for the top1 prediction: tensor([4], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'output CAM.jpg for the top1 prediction: {pred.argmax(1)}')\n",
    "channel, height, width = test_img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + test_img.T * 0.5\n",
    "\n",
    "cv2.imwrite('CAM.jpg',np.hstack([test_img.T, heatmap]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2afe5adcac51d02a2e1812bcf61b8667819276e0f44a7370b1e04a47e62fa1fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
