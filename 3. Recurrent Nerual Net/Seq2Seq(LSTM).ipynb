{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_input = nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.linear_hidden = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        h = self.linear_input(x) + self.linear_hidden(state[0])\n",
    "\n",
    "        chunk_forgetgate, chunk_ingate, chunk_cellgate, chunk_outgate = torch.chunk(h, chunks=4, dim=1)\n",
    "\n",
    "        f_x = torch.sigmoid(chunk_forgetgate)\n",
    "        i_x = torch.sigmoid(chunk_ingate)\n",
    "        c_y = torch.tanh(chunk_cellgate)\n",
    "        o_x = torch.sigmoid(chunk_outgate)\n",
    "\n",
    "        cx = f_x * state[1]\n",
    "        cy = cx + i_x * c_y\n",
    "        hy = o_x * torch.tanh(cy)\n",
    "\n",
    "        return hy, (hy, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self,*cell_args):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.cell = LSTMCell(*cell_args)\n",
    "\n",
    "    def forward(self, x, state, length_x=None):\n",
    "        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n",
    "        inputs = x.unbind(0)\n",
    "        assert (length_x is None) or torch.all(length_x == length_x.sort(descending=True)[0])\n",
    "        outputs = [] \n",
    "        out_hidden_state = []\n",
    "        out_cell_state = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i] , state)\n",
    "            outputs += [out] \n",
    "            if length_x is not None:\n",
    "                if torch.any(i+1 == length_x):\n",
    "                    out_hidden_state = [state[0][i+1==length_x]] + out_hidden_state\n",
    "                    out_cell_state = [state[1][i+1==length_x]] + out_cell_state\n",
    "        if length_x is not None:\n",
    "            state = (torch.cat(out_hidden_state, dim=0), torch.cat(out_cell_state, dim=0))\n",
    "        return torch.stack(outputs), state \n",
    "    \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, ninp, nhid, num_layers, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.layers = []\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(LSTMLayer(ninp, nhid))\n",
    "            else:\n",
    "                self.layers.append(LSTMLayer(nhid, nhid)) \n",
    "        self.layers = nn.ModuleList(self.layers) \n",
    "\n",
    "    def forward(self, x, states, length_x=None):\n",
    "        output_states = []\n",
    "        output = None # (L, B, nhid)\n",
    "\n",
    "        for layer, state in zip(self.layers, states):\n",
    "          if output is None:\n",
    "            output, output_state = layer(x, state, length_x)\n",
    "          else:\n",
    "            output, output_state = layer(output, state, length_x)\n",
    "\n",
    "          output = self.dropout(output)\n",
    "          output_states.append(output_state)\n",
    "        \n",
    "        return output, output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        ninp = args.ninp\n",
    "        nhid = args.nhid\n",
    "        nlayers = args.nlayers\n",
    "        dropout = args.dropout\n",
    "        self.embed = nn.Embedding(src_ntoken, ninp, padding_idx=pad_id)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = LSTM(ninp, nhid, nlayers, dropout)\n",
    "        \n",
    "    def forward(self, x, states, length_x=None):\n",
    "        h = self.embed(x)\n",
    "        h = self.dropout(h)\n",
    "        output, context_vector = self.lstm(h, states, length_x)\n",
    "\n",
    "        return output, context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(trg_ntoken, args.ninp, padding_idx=pad_id)\n",
    "        self.lstm = LSTM(args.ninp, args.nhid, args.nlayers, args.dropout)\n",
    "        self.fc_out = nn.Linear(args.nhid, trg_ntoken)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.fc_out.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, x, states):\n",
    "        h = self.embed(x)\n",
    "        h = self.dropout(h)\n",
    "        output, output_states = self.lstm(h, states)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output, output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.encoder = LSTMEncoder()\n",
    "        self.decoder = LSTMDecoder()\n",
    "    \n",
    "    def _get_init_states(self, x):\n",
    "        init_states = [\n",
    "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
    "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
    "            for _ in range(args.nlayers)\n",
    "        ]\n",
    "        return init_states\n",
    "\n",
    "    def forward(self, x, y, length, max_len=None, teacher_forcing=True):\n",
    "        outputs = []\n",
    "        trg_len = y.size(0) if max_len is None else max_len\n",
    "        \n",
    "        _, output_states = self.encoder(x, self._get_init_states(x), length)\n",
    "        dec_output, output_states = self.decoder(y[0:1], output_states)\n",
    "        outputs.append(dec_output)\n",
    "\n",
    "        for idx in range(trg_len-2):\n",
    "          idx +=1 \n",
    "          if teacher_forcing is True:\n",
    "            dec_output, output_states = self.decoder(y[idx:idx+1], output_states)\n",
    "          else:\n",
    "            dec_output, output_states = self.decoder(dec_output.argmax(-1), output_states)\n",
    "          \n",
    "          outputs.append(dec_output)\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "        return outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28d92f9485c9746d004c1b13759b6237953046e8a4bb9fdcba8f1d233ab2caab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
