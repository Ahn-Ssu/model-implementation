{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = _\n",
    "\n",
    "class MaskedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, mask=False):\n",
    "        super(MaskedMultiheadAttention, self).__init__()\n",
    "        assert args.nhid_tran % args.nhead == 0\n",
    "        self.key = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
    "        self.query = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
    "        self.value = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(args.attn_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        if mask:\n",
    "            self.register_buffer(\"mask\", torch.tril(torch.ones(MAX_LEN, MAX_LEN)))\n",
    "        self.nhead = args.nhead\n",
    "        self.d_k = args.nhid_tran // args.nhead\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q = self.query(q).reshape(q.shape[0], q.shape[1], self.nhead, -1).contiguous().transpose(1,2).contiguous()\n",
    "        k = self.key(k).reshape(k.shape[0], k.shape[1], self.nhead, -1).contiguous().transpose(1,2).contiguous()\n",
    "        v = self.value(v).reshape(v.shape[0], v.shape[1], self.nhead, -1).contiguous().transpose(1,2).contiguous()\n",
    "        \n",
    "        similiarity = torch.matmul(q, k.transpose(-1,-2)) / self.d_k ** 0.5\n",
    "\n",
    "        \n",
    "        # if hasattr(self, 'mask'): \n",
    "        if mask is None:\n",
    "          # similiarity.shape = (B, nhead, T_q, T); \n",
    "          # self.mask.shape = (T_q, T) --> (1, 1, T_q, T)\n",
    "          mask = self.mask[:similiarity.shape[-2], :similiarity.shape[-1]].unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "          mask = mask.repeat(similiarity.shape[0], 1, 1, 1)\n",
    "        else:\n",
    "          # similiarity.shape = (B, nhead, T_q, T)\n",
    "          # mask shape = (B,T)  --> (B, 1, 1, T)\n",
    "          mask = mask.unsqueeze(dim=1).unsqueeze(dim=1)\n",
    "          mask = mask.repeat(1, 1, similiarity.shape[2], 1)\n",
    "        \n",
    "        similiarity = similiarity.masked_fill(mask==0, -np.inf)\n",
    "        scaled = self.attn_drop(torch.softmax(similiarity, dim=-1))\n",
    "        attn_out = torch.matmul(scaled, v).transpose(1,2)\n",
    "        attn_out = attn_out.contiguous().reshape(attn_out.shape[0], attn_out.shape[1], -1)\n",
    "        output = self.proj(attn_out)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerEncLayer, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(args.nhid_tran)\n",
    "        self.ln2 = nn.LayerNorm(args.nhid_tran)\n",
    "        self.attn = MaskedMultiheadAttention()\n",
    "        self.dropout1 = nn.Dropout(args.resid_pdrop)\n",
    "        self.dropout2 = nn.Dropout(args.resid_pdrop)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(args.nhid_tran, args.nff),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(args.nff, args.nhid_tran)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        id1 = self.ln1(x) \n",
    "        attn_out = self.attn(id1, id1, id1, mask)\n",
    "        attn_out = self.dropout1(attn_out)\n",
    "        attn_out = id1 + attn_out\n",
    "        \n",
    "        id2 = self.ln2(attn_out)\n",
    "        h = self.ff(id2)\n",
    "        h = self.dropout2(h)\n",
    "\n",
    "        outputs = id2 + h\n",
    "\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerDecLayer, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(args.nhid_tran)\n",
    "        self.ln2 = nn.LayerNorm(args.nhid_tran)\n",
    "        self.ln3 = nn.LayerNorm(args.nhid_tran)\n",
    "        self.dropout1 = nn.Dropout(args.resid_pdrop)\n",
    "        self.dropout2 = nn.Dropout(args.resid_pdrop)\n",
    "        self.dropout3 = nn.Dropout(args.resid_pdrop)\n",
    "        self.attn1 = MaskedMultiheadAttention(mask=True) # self-attention \n",
    "        self.attn2 = MaskedMultiheadAttention() # tgt to src attention\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(args.nhid_tran, args.nff),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(args.nff, args.nhid_tran)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, enc_o, enc_mask=None):\n",
    "        id1 = self.ln1(x)\n",
    "        attn_out1 = self.attn1(id1, id1, id1)\n",
    "        attn_out1 = self.dropout1(attn_out1)\n",
    "        attn_out1 = attn_out1 + id1\n",
    "\n",
    "        id2 = self.ln2(attn_out1)\n",
    "        attn_out2 = self.attn2(id2, enc_o, enc_o, enc_mask)\n",
    "        attn_out2 = self.dropout2(attn_out2)\n",
    "        attn_out2 = attn_out2 + id2 \n",
    "\n",
    "        id3 = self.ln3(attn_out2)\n",
    "        h = self.ff(id3)\n",
    "        h = self.dropout3(h)\n",
    "        outputs = h + id3 \n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len=4096):\n",
    "        super().__init__()\n",
    "        dim = args.nhid_tran\n",
    "        pos = np.arange(0, max_len)[:, None]\n",
    "        i = np.arange(0, dim // 2)\n",
    "        denom = 10000 ** (2 * i / dim)\n",
    "\n",
    "        pe = np.zeros([max_len, dim])\n",
    "        pe[:, 0::2] = np.sin(pos / denom)\n",
    "        pe[:, 1::2] = np.cos(pos / denom)\n",
    "        pe = torch.from_numpy(pe).float()\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.shape[1]]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(src_ntoken, args.nhid_tran)\n",
    "        self.pos_enc = PositionalEncoding()\n",
    "        self.dropout = nn.Dropout(args.embd_pdrop)\n",
    "        # transformer\n",
    "        self.transform = nn.ModuleList([TransformerEncLayer() for _ in range(args.nlayers_transformer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(args.nhid_tran)\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        emb = self.tok_emb(x)\n",
    "        pos_enc = self.pos_enc(emb)\n",
    "        out = self.dropout(pos_enc)\n",
    "\n",
    "        for iter, transformerLayer in enumerate(self.transform):\n",
    "          out = transformerLayer(out, mask)\n",
    "          \n",
    "        outputs = self.ln_f(out)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.tok_emb = nn.Embedding(trg_ntoken, args.nhid_tran)\n",
    "        self.pos_enc = PositionalEncoding()\n",
    "        self.dropout = nn.Dropout(args.embd_pdrop)\n",
    "        self.transform = nn.ModuleList([TransformerDecLayer() for _ in range(args.nlayers_transformer)])\n",
    "        self.ln_f = nn.LayerNorm(args.nhid_tran)\n",
    "        self.lin_out = nn.Linear(args.nhid_tran, trg_ntoken)\n",
    "        self.lin_out.weight = self.tok_emb.weight\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_o, enc_mask):\n",
    "        emb = self.tok_emb(x)\n",
    "        pos_enc = self.pos_enc(emb)\n",
    "        out = self.dropout(pos_enc)\n",
    "\n",
    "        for iter, transformerLayer in enumerate(self.transform):\n",
    "          out = transformerLayer(out, enc_o, enc_mask)\n",
    "\n",
    "        h = self.ln_f(out)\n",
    "        logits = self.lin_out(h)\n",
    "\n",
    "        logits /= args.nhid_tran ** 0.5 # Scaling logits. Do not modify this\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder()\n",
    "        self.decoder = TransformerDecoder()\n",
    "        \n",
    "    def forward(self, x, y, length_x, max_len=None, teacher_forcing=True):\n",
    "        if max_len is None:\n",
    "          max_len = y.shape[1]\n",
    "\n",
    "        if length_x is not None:\n",
    "          enc_mask = torch.ones(x.shape).to(device)\n",
    "\n",
    "          for idx_i in range(enc_mask.shape[0]):\n",
    "            for idx_j in range(enc_mask.shape[1]):\n",
    "              if length_x[idx_i] > idx_j:\n",
    "                continue\n",
    "              enc_mask[idx_i, idx_j] = 0\n",
    "\n",
    "        enc_o = self.encoder(x, enc_mask)\n",
    "\n",
    "        if teacher_forcing or self.training:\n",
    "          outputs = self.decoder(y[:, :-1], enc_o, enc_mask)\n",
    "\n",
    "          return outputs\n",
    "\n",
    "        else:\n",
    "          dec_input = y[:, :1]\n",
    "          dec_output = None\n",
    "\n",
    "          for iter in range(max_len-1):\n",
    "            dec_output = self.decoder(dec_input, enc_o, enc_mask)\n",
    "            dec_input = torch.cat((dec_input, dec_output[:,-1:].argmax(-1)), dim=1)\n",
    "            \n",
    "          return dec_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28d92f9485c9746d004c1b13759b6237953046e8a4bb9fdcba8f1d233ab2caab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
