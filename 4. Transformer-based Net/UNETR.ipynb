{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info\n",
    "**UNETR: Transformers for 3D Medical Image Segmentation**    \n",
    "*Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, Daguang Xu*   \n",
    "[[paper](https://arxiv.org/abs/2103.10504)]   \n",
    "WACV 2022     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.nets import UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding_UNETR(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_dim, latent_dim) -> None:\n",
    "        super(PatchEmbedding_UNETR, self).__init__()\n",
    "\n",
    "        # N = H*W*D / P^3\n",
    "        self.num_patches = image_size // patch_size\n",
    "        print(self.num_patches)\n",
    "\n",
    "        # project into a K dim\n",
    "        self.project = nn.Linear(in_features= (patch_size**3)*in_dim, out_features=latent_dim)\n",
    "\n",
    "        # positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(self.num_patches, latent_dim))\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Flatten -> Projection -> Position\n",
    "        B, _, _, _, _ = img.shape # (B C D W H)\n",
    "\n",
    "        patches = img.reshape(B, self.num_patch, -1) # (B C D W H) -> (B, N, P^3xC) --- N = (HxWxD)/P^3\n",
    "        patches = self.project(patches) # (B, N, P*P*P*C) -> (B, N, D)\n",
    "        patches = patches + self.pos_embed\n",
    "\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_block(nn.Module):\n",
    "    def __init__(self, latent_dim, num_heads) -> None:\n",
    "        super(Transformer_block, self).__init__()\n",
    "\n",
    "        self.self_attn = nn.Sequential(\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.MultiheadAttention(embed_dim=latent_dim, num_heads=num_heads)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, patches):\n",
    "\n",
    "        residual = patches\n",
    "        patches = self.self_attn(patches)\n",
    "        patches = patches + residual\n",
    "\n",
    "        residual = patches\n",
    "        patches = self.ff(patches)\n",
    "        patches = patches + residual\n",
    "\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNETR_encoder(nn.Module):\n",
    "    def __init__(self, in_dim, d_model, image_size, patch_size, num_heads) -> None:\n",
    "        super(UNETR_encoder, self).__init__()\n",
    "        self.num_patches = image_size // patch_size\n",
    "\n",
    "        self.patch_emb = PatchEmbedding_UNETR(in_dim=in_dim, latent_dim=d_model, image_size=image_size, patch_size=patch_size)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            *[Transformer_block(latent_dim=d_model, num_heads=num_heads) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            *[Transformer_block(latent_dim=d_model, num_heads=num_heads) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            *[Transformer_block(latent_dim=d_model, num_heads=num_heads) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            *[Transformer_block(latent_dim=d_model, num_heads=num_heads) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        B, C, D, H, W = x.shape\n",
    "        stage_outputs = {}\n",
    "\n",
    "        x = self.patch_emb(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        stage_outputs['z3'] = x.reshape(B, -1, D/self.num_patches, H/self.num_patches, W/self.num_patches)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        stage_outputs['z6'] = x.reshape(B, -1, D/self.num_patches, H/self.num_patches, W/self.num_patches)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        stage_outputs['z9'] = x.reshape(B, -1, D/self.num_patches, H/self.num_patches, W/self.num_patches)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        stage_outputs['z12'] = x.reshape(B, -1, D/self.num_patches, H/self.num_patches, W/self.num_patches)\n",
    "\n",
    "        return stage_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class UNETR_skipconnection(nn.Module):\n",
    "    def __init__(self, in_dim:int, hidden_dim:Sequence[int]):\n",
    "        super(UNETR_skipconnection, self).__init__()\n",
    "        \n",
    "        self.skip1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_dim, out_channels=hidden_dim[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=hidden_dim[0], out_channels=hidden_dim[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.skip2 = nn.Sequential(\n",
    "            # 768 -> 512\n",
    "            nn.ConvTranspose3d(in_channels=hidden_dim[4], out_channels=hidden_dim[3], kernel_size=2, stride=2),\n",
    "            nn.Conv3d(in_channels=hidden_dim[3], out_channels=hidden_dim[3], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[3]),\n",
    "            nn.ReLU(),\n",
    "            # 512 -> 256\n",
    "            nn.ConvTranspose3d(in_channels=hidden_dim[3], out_channels=hidden_dim[2], kernel_size=2, stride=2),\n",
    "            nn.Conv3d(in_channels=hidden_dim[2], out_channels=hidden_dim[2], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[2]),\n",
    "            nn.ReLU(),\n",
    "            # 256 -> 128\n",
    "            nn.ConvTranspose3d(in_channels=hidden_dim[2], out_channels=hidden_dim[1], kernel_size=2, stride=2),\n",
    "            nn.Conv3d(in_channels=hidden_dim[1], out_channels=hidden_dim[1], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.skip3 = nn.Sequential(\n",
    "            # 768 -> 512\n",
    "            nn.ConvTranspose3d(in_channels=hidden_dim[4], out_channels=hidden_dim[3], kernel_size=2, stride=2),\n",
    "            nn.Conv3d(in_channels=hidden_dim[3], out_channels=hidden_dim[3], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[3]),\n",
    "            nn.ReLU(),\n",
    "            # 512 -> 256\n",
    "            nn.ConvTranspose3d(in_channels=hidden_dim[3], out_channels=hidden_dim[2], kernel_size=2, stride=2),\n",
    "            nn.Conv3d(in_channels=hidden_dim[2], out_channels=hidden_dim[2], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[2]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.skip4 = nn.Sequential(\n",
    "            # 768 -> 512\n",
    "            nn.ConvTranspose3d(in_channels=hidden_dim[4], out_channels=hidden_dim[3], kernel_size=2, stride=2),\n",
    "            nn.Conv3d(in_channels=hidden_dim[3], out_channels=hidden_dim[3], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[3]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, stage_outputs:dict):\n",
    "\n",
    "        stage_outputs['z0'] = self.skip1(x)\n",
    "        stage_outputs['z3'] = self.skip2(stage_outputs['z3'])\n",
    "        stage_outputs['z6'] = self.skip3(stage_outputs['z6'])\n",
    "        stage_outputs['z9'] = self.skip4(stage_outputs['z9'])\n",
    "\n",
    "        return stage_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNETR_decoder(nn.Module):\n",
    "    def __init__(self, out_dim:int, hidden_dim:Sequence[int]) -> None:\n",
    "        super(UNETR_decoder, self).__init__()\n",
    "\n",
    "        self.up1 = nn.ConvTranspose3d(in_channels=hidden_dim[4], out_channels=hidden_dim[3], kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=hidden_dim[3]*2, out_channels=hidden_dim[3], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=hidden_dim[3], out_channels=hidden_dim[3], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[3]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose3d(in_channels=hidden_dim[3], out_channels=hidden_dim[2], kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=hidden_dim[2]*2, out_channels=hidden_dim[2], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=hidden_dim[2], out_channels=hidden_dim[2], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[2]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose3d(in_channels=hidden_dim[2], out_channels=hidden_dim[1], kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=hidden_dim[1]*2, out_channels=hidden_dim[1], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=hidden_dim[1], out_channels=hidden_dim[1], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up4 = nn.ConvTranspose3d(in_channels=hidden_dim[1], out_channels=hidden_dim[0], kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=hidden_dim[0]*2, out_channels=hidden_dim[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=hidden_dim[0], out_channels=hidden_dim[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(hidden_dim[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Conv3d(in_channels=hidden_dim[0], out_channels=out_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, stage_outputs:dict):\n",
    "\n",
    "        z = stage_outputs['z12']\n",
    "        z = self.up1(z)\n",
    "        z = torch.concat([z, stage_outputs['z9']], dim=1)\n",
    "        z = self.conv1(z)\n",
    "\n",
    "        z = self.up2(z)\n",
    "        z = torch.concat([z, stage_outputs['z6']], dim=1)\n",
    "        z = self.conv2(z)\n",
    "\n",
    "        z = self.up3(z)\n",
    "        z = torch.concat([z, stage_outputs['z3']], dim=1)\n",
    "        z = self.conv3(z)\n",
    "\n",
    "        z = self.up4(z)\n",
    "        z = torch.concat([z, stage_outputs['z0']], dim=1)\n",
    "        z = self.conv4(z)\n",
    "\n",
    "        z = self.conv5(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNETR(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim:Sequence[int], img_size, patch_size, num_heads=16, d_model=768) -> None:\n",
    "        super(UNETR, self).__init__()\n",
    "\n",
    "        self.encoder = UNETR_encoder(in_dim=in_dim, d_model=d_model, image_size=img_size, patch_size=patch_size, num_heads=num_heads)\n",
    "        self.skipnet = UNETR_skipconnection(in_dim=in_dim, hidden_dim=hidden_dim)\n",
    "        self.docoder = UNETR_decoder(out_dim=out_dim, hidden_dim=hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        stage_outputs = self.encoder(x)\n",
    "        stage_outputs = self.skipnet(x, stage_outputs)\n",
    "        out = self.decoder(stage_outputs)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m UNETR(\n\u001b[0;32m      2\u001b[0m     in_dim\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m     out_dim\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     hidden_dim\u001b[39m=\u001b[39;49m[\u001b[39m64\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m256\u001b[39;49m, \u001b[39m512\u001b[39;49m, \u001b[39m768\u001b[39;49m],\n\u001b[0;32m      5\u001b[0m     img_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     patch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     num_heads\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     d_model\u001b[39m=\u001b[39;49m\u001b[39m768\u001b[39;49m\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m, in \u001b[0;36mUNETR.__init__\u001b[1;34m(self, in_dim, out_dim, hidden_dim, img_size, patch_size, num_heads, d_model)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39msuper\u001b[39m(UNETR, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m UNETR_encoder(in_dim\u001b[39m=\u001b[39min_dim, d_model\u001b[39m=\u001b[39md_model, image_size\u001b[39m=\u001b[39mimg_size, patch_size\u001b[39m=\u001b[39mpatch_size, num_heads\u001b[39m=\u001b[39mnum_heads)\n\u001b[1;32m----> 6\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipnet \u001b[39m=\u001b[39m UNETR_skipconnection(in_dim\u001b[39m=\u001b[39;49min_dim, hidden_dim\u001b[39m=\u001b[39;49mhidden_dim)\n\u001b[0;32m      7\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocoder \u001b[39m=\u001b[39m UNETR_decoder(out_dim\u001b[39m=\u001b[39mout_dim, hidden_dim\u001b[39m=\u001b[39mhidden_dim)\n",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m, in \u001b[0;36mUNETR_skipconnection.__init__\u001b[1;34m(self, in_dim, hidden_dim)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_dim:\u001b[39mint\u001b[39m, hidden_dim:Sequence[\u001b[39mint\u001b[39m]):\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m      7\u001b[0m         nn\u001b[39m.\u001b[39mConv3d(in_channels\u001b[39m=\u001b[39min_dim, out_channels\u001b[39m=\u001b[39mhidden_dim[\u001b[39m0\u001b[39m], kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m      8\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm3d(hidden_dim[\u001b[39m0\u001b[39m]),\n\u001b[0;32m      9\u001b[0m         nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m     10\u001b[0m         nn\u001b[39m.\u001b[39mConv3d(in_channels\u001b[39m=\u001b[39mhidden_dim[\u001b[39m0\u001b[39m], out_channels\u001b[39m=\u001b[39mhidden_dim[\u001b[39m0\u001b[39m], kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m     11\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm3d(hidden_dim[\u001b[39m0\u001b[39m]),\n\u001b[0;32m     12\u001b[0m         nn\u001b[39m.\u001b[39mReLU()\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     16\u001b[0m         \u001b[39m# 768 -> 512\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         nn\u001b[39m.\u001b[39mConvTranspose3d(in_channels\u001b[39m=\u001b[39mhidden_dim[\u001b[39m4\u001b[39m], out_channels\u001b[39m=\u001b[39mhidden_dim[\u001b[39m3\u001b[39m], kernel_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m         nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m     31\u001b[0m     )\n\u001b[0;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip3 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     34\u001b[0m         \u001b[39m# 768 -> 512\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         nn\u001b[39m.\u001b[39mConvTranspose3d(in_channels\u001b[39m=\u001b[39mhidden_dim[\u001b[39m4\u001b[39m], out_channels\u001b[39m=\u001b[39mhidden_dim[\u001b[39m3\u001b[39m], kernel_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m         nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m     44\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Suhyun\\.conda\\envs\\Vision\\lib\\site-packages\\torch\\nn\\modules\\module.py:1298\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Module):\n\u001b[0;32m   1297\u001b[0m     \u001b[39mif\u001b[39;00m modules \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m   1299\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcannot assign module before Module.__init__() call\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1300\u001b[0m     remove_from(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_non_persistent_buffers_set)\n\u001b[0;32m   1301\u001b[0m     modules[name] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "model = UNETR(\n",
    "    in_dim=4,\n",
    "    out_dim=3,\n",
    "    hidden_dim=[64, 128, 256, 512, 768],\n",
    "    img_size=128,\n",
    "    patch_size=8,\n",
    "    num_heads=16,\n",
    "    d_model=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
